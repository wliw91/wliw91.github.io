---
title: python信息收集(子域名+邮箱)
date: 2021-08-28
categories: 
- python
tags: 
- python 信息收集
---

本文主要描述使用python进行信息收集包括收集子域名和邮箱

<!-- more -->

#### 概述

被动信息搜集主要通过搜索引擎或者社交等方式对目标资产信息进行提取，通常包括IP查询，Whois查询，子域名搜集等。进行被动信息搜集时不与目标产生交互，可以在不接触到目标系统的情况下挖掘目标信息。

主要方法：DNS解析，子域名挖掘，邮件爬取等。

#### DNS解析

##### **1、概述**

DNS（Domain Name System，域名系统）是一种分布式网络目录服务，主要用于域名与IP地址的相互转换，能够使用户更方便地访问互联网，而不用去记住一长串数字（能够被机器直接读取的IP）。

##### **2、IP查询**

IP查询是通过当前所获取的URL去查询对应IP地址的过程。可以利用Socket库函数中的gethostbyname()获取域名对应的IP值。

代码：

```
#IP解析
import socket
url="tool.chinaz.com"
print("需要扫描的URL为："+url)
ip = socket.gethostbyname(url)
print("解析IP地址信息："+ip)
res = socket.getaddrinfo(url,None)
for i in range(0,len(res)):
    print(res[i][4][0])
```

输出：

```
需要扫描的URL为：www.cnblogs.com
解析IP地址信息：101.37.115.180
101.37.115.180
114.55.205.139
114.55.187.58
101.37.113.127
116.62.84.58
```

##### **3、Whois查询**

Whois是用来查询域名的IP以及所有者信息的传输协议。Whois相当于一个数据库，用来查询域名是否已经被注册，以及注册域名的详细信息（如域名所有人，域名注册商等）。

Python中的python-whois模块可用于Whois查询。

代码

```
#使用whois模块查询信息
from whois import whois
data=whois('baidu.com')#只能是域名
print(data)
```

输出

```
{
  "domain_name": [
    "BAIDU.COM",
    "baidu.com"
  ],
  "registrar": "MarkMonitor, Inc.",
  "whois_server": "whois.markmonitor.com",
  "referral_url": null,
  "updated_date": [
    "2020-12-09 04:04:41",
    "2021-04-07 12:52:21-07:00"
  ],
  "creation_date": [
    "1999-10-11 11:05:17",
    "1999-10-11 04:05:17-07:00"
  ],
  "expiration_date": [
    "2026-10-11 11:05:17",
    "2026-10-11 00:00:00-07:00"
  ],
  "name_servers": [
    "NS1.BAIDU.COM",
    "NS2.BAIDU.COM",
    "NS3.BAIDU.COM",
    "NS4.BAIDU.COM",
    "NS7.BAIDU.COM",
    "ns2.baidu.com",
    "ns3.baidu.com",
    "ns1.baidu.com",
    "ns4.baidu.com",
    "ns7.baidu.com"
  ],
  "status": [
    "clientDeleteProhibited https://icann.org/epp#clientDeleteProhibited",
    "clientTransferProhibited https://icann.org/epp#clientTransferProhibited",
    "clientUpdateProhibited https://icann.org/epp#clientUpdateProhibited",
    "serverDeleteProhibited https://icann.org/epp#serverDeleteProhibited",
    "serverTransferProhibited https://icann.org/epp#serverTransferProhibited",
    "serverUpdateProhibited https://icann.org/epp#serverUpdateProhibited",
    "clientUpdateProhibited (https://www.icann.org/epp#clientUpdateProhibited)",
    "clientTransferProhibited (https://www.icann.org/epp#clientTransferProhibited)",
    "clientDeleteProhibited (https://www.icann.org/epp#clientDeleteProhibited)",
    "serverUpdateProhibited (https://www.icann.org/epp#serverUpdateProhibited)",
    "serverTransferProhibited (https://www.icann.org/epp#serverTransferProhibited)",
    "serverDeleteProhibited (https://www.icann.org/epp#serverDeleteProhibited)"
  ],
  "emails": [
    "abusecomplaints@markmonitor.com",
    "whoisrequest@markmonitor.com"
  ],
  "dnssec": "unsigned",
  "name": null,
  "org": "Beijing Baidu Netcom Science Technology Co., Ltd.",
  "address": null,
  "city": null,
  "state": "Beijing",
  "zipcode": null,
  "country": "CN"
}
```

#### 子域名挖掘

##### **1、概述**

域名可以分为顶级域名，一级域名，二级域名等。

子域名（subdomain）是顶级域名（一级域名或父域名）的下一级。

在测试过程中，测试目标主站时如果未发现任何相关漏洞，此时通常会考虑挖掘目标系统的子域名。

子域名挖掘方法有多种，例如，搜索引擎，子域名破解，字典查询等。

##### **2、利用Python编写一个简单的子域名挖掘工具**

代码

```
#通过https://cn.bing.com爬取site的前pages页的搜索结果
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse

def bing_search(site,pages):
    sudbomain=[]
    headers={'User-Agent':'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)',
        'Accept':'*/*',
    }
    for i in range(1,int(pages)+1):
        url="https://cn.bing.com/search?q={0}&sp=-1&pq={0}&sc=8-9&qs=n&sk=" \
            "&cvid=C5D574DF2A5943C294A7DA02DFF96552&first={1}&FORM=PERE".format(site,str((int(pages)-1)*10))
        conn=requests.session()
        try:
            conn.get('https://cn.bing.com',headers=headers)
            html = conn.get(url=url,stream=True,headers=headers,timeout=8)
            soup =BeautifulSoup(html.content,'html.parser')
            job_bt=soup.findAll('h2')
            for i in job_bt:
                link=i.a.get('href')
                domain=str(urlparse(link).scheme+"://"+urlparse(link).netloc)
                if domain in sudbomain:
                    pass
                else:
                    sudbomain.append(domain)
                    print(domain)
        except Exception as e:
            print(e)
            print(link)
if __name__ == '__main__':
    site="baidu.com"
    pages=15
    bing_search(site, pages)
```

输出

```
http://www.baidu.com
http://www2.baidu.com
http://quanjing.baidu.com
https://huiyan.baidu.com
https://quanmin.baidu.com
https://bgc.map.baidu.com
https://bsrc.baidu.com
https://srf.baidu.com
http://daishu.baidu.com
https://smartprogram.baidu.com
'NoneType' object has no attribute 'get'
https://smartprogram.baidu.com/developer/index.html
```

#### 邮件爬取

##### 1、概述

在针对目标系统进行渗透的过程中，如果目标服务器安全性很高，通过服务器很难获取目标权限时，通常会采用社工的方式对目标服务进行进一步攻击。

针对搜索界面的相关邮件信息进行爬取、处理等操作之后。利用获得的邮箱账号批量发送钓鱼邮件，诱骗、欺诈目标用户或管理员进行账号登录或点击执行，进而获取目标系统的其权限。

##### 2、说明

（1）确定搜索邮件的关键字，并调用bing_search()和baidu_search()两个函数，返回Bing与百度两大搜索引擎的查询结果。由获取到的结果进行列表合并，去重之后，循环输出。

（2）用Bing搜索引擎进行邮件爬取。Bing引擎具有反爬防护，会通过限定referer、cookie等信息来确定是否网页爬取操作。可以通过指定referer与requeses.session()函数自动获取cookie信息，绕过Bing搜索引擎的反爬防护。

（3）用百度搜索引擎进行邮件爬取。百度搜索引擎同样设定了反爬防护，相对Bing来说，百度不仅对referer和cookie进行校验，还同时在页面中通过JavaScript语句进行动态请求链接，从而导致不能动态获取页面中的信息。可以通过对链接的提取，在进行request请求，从而绕过反爬设置。

（4）通过正则表达式获取邮箱号码。此处也可以换成目标企业邮箱的正则表达式。

代码

```
import getopt
import requests
from bs4 import BeautifulSoup
import re
def headers(referer):
    headers = {
        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36',
        'Accept': 'application/json, text/javascript, */*; q=0.01',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Referer': referer
    }
    return headers
def launcher(url, pages):
    email_num=[]
    key_words=['email','mail','mailbos','邮件','邮箱','postbox']
    for page in range(1,int(pages)+1):
        for key_word in key_words:
            bing_emails=bing_search(url,page,key_word)
            baidu_emails=baidu_search(url, page, key_word)
            sum_emails=bing_emails+baidu_emails
            for email in sum_emails:
                if email in email_num:
                    pass
                else:
                    print(email)
def bing_search(url, page, key_word):
    referer = "http://cn.bing.com/search?q=email+site%3abaidu.com&sp=-1&pq=emailsite%3abaidu.com&first=1&FORM=PERE1"
    conn = requests.session()
    bing_url = "http://cn.bing.com/search?q=" + key_word + "+site%3a" + url + "&qa=n&sp=-1&pq=" + key_word + "site%3a" + url + "&first=" + str(
        (page - 1) * 10) + "&FORM=PERE1"
    conn.get('http://cn.bing.com', headers=headers(referer))
    r = conn.get(bing_url, stream=True, headers=headers(referer), timeout=8)
    emails = search_email(r.text)
    return emails
def baidu_search(url, page, key_word):
    email_list = []
    emails = []
    referer = "https://www.baidu.com/s?wd=email+site%3Abaidu.com&pn=1"
    baidu_url = "https://www.baidu.com/s?wd=" + key_word + "+site%3A" + url + "&pn=" + str((page - 1) * 10)
    conn = requests.session()
    conn.get(baidu_url, headers=headers(referer))
    r = conn.get(baidu_url, headers=headers(referer))
    soup = BeautifulSoup(r.text, 'lxml')
    tagh3 = soup.find_all('h3')
    for h3 in tagh3:
        href = h3.find('a').get('href')
        try:
            r = requests.get(href, headers=headers(referer))
            emails = search_email(r.text)
        except Exception as e:
            pass
        for email in emails:
            email_list.append(email)
    return email_list
def search_email(html):
    emails=re.findall(r"[a-z0-9\.\-+_]+@[a-z0-9\.\-+_]+\.[a-z]+",html,re.I)
    return emails
if __name__ == '__main__':
    url = "baidu.com"
    pages = 1
    launcher(url, pages)
```

输出

```
abcdefg@mail.com
abcdefg@sina.com
abcdefg@163.com
abcdefg@sohu.com
abcdefg@qq.com
```

#### 参考链接

https://www.cnblogs.com/3cH0-Nu1L/p/14698655.html

https://www.cnblogs.com/ms08067/
